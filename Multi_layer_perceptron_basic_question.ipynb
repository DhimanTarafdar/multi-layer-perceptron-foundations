{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcfJvoRXDxVo0gKwfEctWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhimanTarafdar/AAA/blob/main/Multi_layer_perceptron_basic_question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.What happens if we stack multiple linear layers without activation?"
      ],
      "metadata": {
        "id": "yf93Yj2EfsVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# কেন আমরা Deep Neural Network-এ শুধুমাত্র Linear Layer স্ট্যাক করি না?\n",
        "\n",
        "আমরা যখন ডিপ লার্নিং নিয়ে কাজ করি, তখন আমাদের মনে প্রশ্ন জাগতে পারে যে আমরা যদি অ্যাক্টিভেশন ফাংশন ছাড়া অনেকগুলো লিনিয়ার লেয়ার (Hidden Layers) একটার ওপর একটা বসাই, তবে কী হবে? বাস্তবে, **অ্যাক্টিভেশন ফাংশন ছাড়া মাল্টি-লেয়ার নেটওয়ার্ক আসলে একটি সিঙ্গেল লেয়ার নেটওয়ার্কের মতোই কাজ করে।**\n",
        "\n",
        "### ১. গাণিতিক বিশ্লেষণ (Mathematical Intuition)\n",
        "\n",
        "ধরি আমার একটি নেটওয়ার্ক আছে যার দুটি লেয়ার এবং কোনো অ্যাক্টিভেশন ফাংশন নেই।\n",
        "* প্রথম লেয়ারের আউটপুট: $h_1 = W_1 \\cdot X + b_1$\n",
        "* দ্বিতীয় লেয়ারের আউটপুট: $y = W_2 \\cdot h_1 + b_2$\n",
        "\n",
        "এখন যদি আমি $h_1$ এর মান দ্বিতীয় ইকুয়েশনে বসাই:\n",
        "$$y = W_2 \\cdot (W_1 \\cdot X + b_1) + b_2$$\n",
        "$$y = (W_2 \\cdot W_1) \\cdot X + (W_2 \\cdot b_1 + b_2)$$\n",
        "\n",
        "এখানে আমরা দেখতে পাচ্ছি যে, $(W_2 \\cdot W_1)$ গুণ হয়ে একটি নতুন ওয়েট ম্যাট্রিক্স $W_{new}$ তৈরি করছে এবং $(W_2 \\cdot b_1 + b_2)$ মিলে একটি নতুন বায়াস $b_{new}$ তৈরি করছে।\n",
        "আলটিমেটলি সমীকরণটি দাঁড়াচ্ছে:\n",
        "$$y = W_{new} \\cdot X + b_{new}$$\n",
        "\n",
        "এটি একটি সাধারণ **Linear Equation** বা **Single Layer Perceptron**-এর সমীকরণ। অর্থাৎ, আমরা মাঝখানে যতগুলো লেয়ারই দিই না কেন, গাণিতিকভাবে সেগুলো মিলেমিশে দিনশেষে একটি লেয়ারেই পরিণত হচ্ছে।\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ২. এটি কেন আমাদের জন্য সমস্যা?\n",
        "\n",
        "* **জটিল প্যাটার্ন শিখতে না পারা:** লিনিয়ার লেয়ার শুধুমাত্র সরলরেখা (Straight line) তৈরি করতে পারে। কিন্তু বাস্তব জগতের ডেটা (যেমন ইমেজ বা টেক্সট) অত্যন্ত জটিল এবং নন-লিনিয়ার। অ্যাক্টিভেশন ফাংশন ছাড়া আমাদের মডেল এই জটিলতা বুঝতেই পারবে না।\n",
        "* **নেটওয়ার্কের গভীরতা হারানো:** আমরা নেটওয়ার্ককে 'Deep' করি যেন প্রতি লেয়ারে নতুন নতুন বৈশিষ্ট্য (Features) শিখতে পারে। অ্যাক্টিভেশন ফাংশন না থাকলে লেয়ার বাড়িয়ে আমাদের কোনো লাভ নেই, কারণ মডেলটি লিনিয়ার রিগ্রেশন হিসেবেই আটকে থাকবে।\n",
        "\n",
        "---\n",
        "\n",
        "### ৩. সহজ উদাহরণ (Intuition)\n",
        "\n",
        "আমরা যদি অনেকগুলো স্বচ্ছ কাঁচের ওপর সোজা দাগ টানি এবং কাঁচগুলোকে একটার ওপর একটা রাখি, তবে আমরা দিনশেষে শুধুমাত্র সোজা দাগই দেখতে পাব। আমরা কখনোই একটি বৃত্ত বা আঁকাবাঁকা ম্যাপ তৈরি করতে পারব না। আমাদের এমন কিছু দরকার যা ওই সোজা দাগগুলোকে বাঁকিয়ে জটিল নকশা তৈরি করতে পারে। অ্যাক্টিভেশন ফাংশন ঠিক এই কাজটিই করে।\n",
        "\n",
        "---\n",
        "\n",
        "### সারসংক্ষেপ (Summary):\n",
        "১. **Collapsing Layers:** সব লেয়ার মিলে একটি মাত্র লিনিয়ার লেয়ারে পরিণত হয়।\n",
        "২. **Linearity:** আমরা শুধুমাত্র লিনিয়ার সমস্যা সমাধান করতে পারি।\n",
        "৩. **Useless Complexity:** লেয়ার বাড়ানো সত্ত্বেও আমাদের মডেলের ক্ষমতা বাড়ে না।\n",
        "\n",
        "**এই কারণেই আমাদের প্রতিটি লেয়ারের পর একটি Non-linear Activation Function (যেমন: ReLU বা Sigmoid) ব্যবহার করা আবশ্যিক।**"
      ],
      "metadata": {
        "id": "D1xeLsaIfx__"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DxfeA1HSfuhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What kind of decision boundary does an MLP create?"
      ],
      "metadata": {
        "id": "QR61nqmngbcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP কেমন Decision Boundary তৈরি করে?\n",
        "\n",
        "আমরা জানি যে একটি সিঙ্গেল লেয়ার পারসেপট্রন শুধুমাত্র একটি সরলরেখা (Straight Line) টানতে পারে। কিন্তু যখন আমরা **Multi-Layer Perceptron (MLP)** ব্যবহার করি এবং তাতে নন-লিনিয়ার অ্যাক্টিভেশন ফাংশন যোগ করি, তখন এটি অনেক বেশি শক্তিশালী হয়ে ওঠে।\n",
        "\n",
        "### ১. নন-লিনিয়ার এবং জটিল বাউন্ডারি (Non-linear & Complex Boundaries)\n",
        "\n",
        "সিঙ্গেল লেয়ারের সীমাবদ্ধতা কাটিয়ে MLP আমাদের এমন সব **Decision Boundary** তৈরি করতে দেয় যা আঁকাবাঁকা বা কার্ভড (Curved) হতে পারে। এটি ডেটার জটিল বিন্যাস অনুযায়ী নিজেকে মানিয়ে নিতে পারে।\n",
        "\n",
        "* **সরলরেখার বদলে বক্ররেখা:** MLP একাধিক লিনিয়ার বাউন্ডারিকে একত্রিত করে একটি জটিল বাউন্ডারি তৈরি করে।\n",
        "* **Universal Approximation Theorem:** এই থিওরি অনুযায়ী, যথেষ্ট পরিমাণ নিউরন এবং অন্তত একটি হিডেন লেয়ার থাকলে একটি MLP প্রায় যেকোনো জটিল গাণিতিক ফাংশন বা প্যাটার্নকে রিপ্রেজেন্ট করতে পারে।\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ২. এটি কীভাবে কাজ করে? (Intuition)\n",
        "\n",
        "আমি বিষয়টি এভাবে চিন্তা করতে পারি:\n",
        "১. **প্রথম হিডেন লেয়ার:** এই লেয়ারের প্রতিটি নিউরন এক একটি সরলরেখা (Linear boundary) তৈরি করে।\n",
        "২. **পরবর্তী লেয়ারসমূহ:** এই সরলরেখাগুলোকে একে অপরের সাথে যুক্ত করে (Combine করে) বিভিন্ন জটিল আকৃতি যেমন—ত্রিভুজ, বৃত্ত বা আরও আঁকাবাঁকা প্যাটার্ন তৈরি করা হয়।\n",
        "\n",
        "আমরা যদি XOR গেটের কথা ভাবি, সেখানে একটি সরলরেখা দিয়ে ডেটা আলাদা করা অসম্ভব। কিন্তু MLP সেখানে দুটি রেখা ব্যবহার করে এবং সেগুলোকে কম্বাইন করে সঠিক সিদ্ধান্ত নিতে পারে।\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ৩. গভীরতার প্রভাব (Impact of Layers)\n",
        "\n",
        "* **কম লেয়ার:** যদি লেয়ার কম থাকে, তবে ডিসিশন বাউন্ডারি কিছুটা সহজ বা কম আঁকাবাঁকা হয়।\n",
        "* **বেশি লেয়ার (Deep Network):** নেটওয়ার্ক যত গভীর হয়, ডিসিশন বাউন্ডারি তত বেশি সূক্ষ্ম এবং জটিল হয়। এটি অত্যন্ত কঠিন প্যাটার্নও শনাক্ত করতে পারে। তবে খুব বেশি জটিল হয়ে গেলে আবার **Overfitting** হওয়ার সম্ভাবনা থাকে।\n",
        "\n",
        "---\n",
        "\n",
        "### সারসংক্ষেপ (Summary):\n",
        "\n",
        "* **Single Layer:** শুধুমাত্র লিনিয়ার (Linear) ডিসিশন বাউন্ডারি তৈরি করে।\n",
        "* **MLP:** নন-লিনিয়ার (Non-linear), কার্ভড এবং অত্যন্ত জটিল (Complex) ডিসিশন বাউন্ডারি তৈরি করতে সক্ষম।\n",
        "* **অ্যাক্টিভেশন ফাংশনের ভূমিকা:** অ্যাক্টিভেশন ফাংশন ছাড়া MLP কখনোই এই বাঁকানো বা জটিল বাউন্ডারি তৈরি করতে পারত না, তখন এটি কেবল লিনিয়ারই থেকে যেত।"
      ],
      "metadata": {
        "id": "MpGZ7G6NguXD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRSf79W4gc6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. What are the main components of a neural network?"
      ],
      "metadata": {
        "id": "82qkoWhjhDUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# নিউরাল নেটওয়ার্কের মূল উপাদানসমূহ (Main Components of a Neural Network)\n",
        "\n",
        "একটি নিউরাল নেটওয়ার্ক মূলত মানুষের মস্তিষ্কের নিউরন সিস্টেমের একটি গাণিতিক রূপ। আমি যদি একটি স্ট্যান্ডার্ড নিউরাল নেটওয়ার্ক বা **Multi-Layer Perceptron (MLP)** এর দিকে তাকাই, তবে এর প্রধান উপাদানগুলোকে নিচে দেওয়া ভাগে ভাগ করতে পারি:\n",
        "\n",
        "### ১. লেয়ারসমূহ (The Layers)\n",
        "\n",
        "একটি নেটওয়ার্কে প্রধানত তিন ধরনের লেয়ার থাকে:\n",
        "* **ইনপুট লেয়ার (Input Layer):** এটি নেটওয়ার্কের প্রথম লেয়ার। আমাদের ডেটা বা ফিচারগুলো (যেমন: ছবির পিক্সেল বা বয়স, উচ্চতা ইত্যাদি) এই লেয়ারের মাধ্যমেই ভেতরে ঢোকে। এখানে কোনো গাণিতিক হিসাব হয় না।\n",
        "* **হিডেন লেয়ার (Hidden Layers):** ইনপুট এবং আউটপুট লেয়ারের মাঝখানের লেয়ারগুলোই হলো হিডেন লেয়ার। নেটওয়ার্কের আসল শেখার কাজ বা জটিল প্যাটার্ন খুঁজে বের করার কাজ এখানেই হয়। নেটওয়ার্ক যত গভীর হয়, হিডেন লেয়ার তত বেশি থাকে।\n",
        "* **আউটপুট লেয়ার (Output Layer):** সবশেষে মডেল যে প্রেডিকশন বা ফলাফল দেয়, তা এই লেয়ার থেকে আসে। ক্লাসিফিকেশন সমস্যার ক্ষেত্রে এখানে সাধারণত আমরা প্রোবাবিলিটি পাই।\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ২. নিউরন বা নোড (Neurons / Nodes)\n",
        "\n",
        "প্রতিটি লেয়ার ছোট ছোট একক দিয়ে গঠিত, যাদের আমরা **নিউরন** বা **নোড** বলি। প্রতিটি নিউরন মূলত একটি গাণিতিক ফাংশন হিসেবে কাজ করে। এটি আগের লেয়ার থেকে তথ্য গ্রহণ করে, সেটিকে প্রসেস করে এবং পরের লেয়ারে পাঠিয়ে দেয়।\n",
        "\n",
        "---\n",
        "\n",
        "### ৩. ওয়েট এবং বায়াস (Weights and Biases)\n",
        "\n",
        "এগুলো হলো নেটওয়ার্কের সেই প্যারামিটার যা আমরা ট্রেইনিংয়ের সময় শিখি:\n",
        "* **ওয়েট ($w$):** ইনপুট কতটা গুরুত্বপূর্ণ তা ওয়েট দিয়ে নির্ধারিত হয়। একটি নিউরন থেকে অন্য নিউরনের কানেকশন কতটা শক্তিশালী হবে তা এটি নিয়ন্ত্রণ করে।\n",
        "* **বায়াস ($b$):** এটি ইনপুটগুলোর সাথে একটি অতিরিক্ত মান হিসেবে যোগ হয়। এটি মডেলকে ডেটার সাথে আরও ভালোভাবে খাপ খাইয়ে নিতে (Fit করতে) এবং ডিসিশন বাউন্ডারিকে ডানে-বামে বা ওপরে-নিচে সরাতে সাহায্য করে।\n",
        "\n",
        "**মৌলিক সমীকরণ:** $$z = (w_1x_1 + w_2x_2 + ... + w_nx_n) + b$$\n",
        "\n",
        "---\n",
        "\n",
        "### ৪. অ্যাক্টিভেশন ফাংশন (Activation Function)\n",
        "\n",
        "আমি যখন লিনিয়ার হিসাব ($z$) পাই, তখন সেটিকে একটি নন-লিনিয়ার রূপ দেওয়ার জন্য অ্যাক্টিভেশন ফাংশন ব্যবহার করি। এটিই ঠিক করে দেয় যে একটি নিউরন পরবর্তী লেয়ারে তথ্য পাঠাবে (Fire করবে) কি না।\n",
        "* **উদাহরণ:** ReLU, Sigmoid, Tanh, Softmax.\n",
        "\n",
        "\n",
        "\n",
        "[Image of a biological neuron versus an artificial neuron]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ৫. লস ফাংশন (Loss Function)\n",
        "\n",
        "মডেলের করা প্রেডিকশন এবং আসল উত্তরের মধ্যে তফাত কতটা, তা মাপার জন্য আমি লস ফাংশন ব্যবহার করি। আমাদের লক্ষ্য থাকে এই লস বা ভুলকে কমিয়ে আনা।\n",
        "* **উদাহরণ:** Binary Cross Entropy (BCE), Mean Squared Error (MSE).\n",
        "\n",
        "---\n",
        "\n",
        "### ৬. অপটিমাইজার (Optimizer)\n",
        "\n",
        "অপটিমাইজার হলো সেই অ্যালগরিদম যা লস ফাংশনের ওপর ভিত্তি করে ওয়েট এবং বায়াসের মান পরিবর্তন করে। এটিই আমাদের শেখায় যে লস কমানোর জন্য প্যারামিটারগুলো কোন দিকে আপডেট করতে হবে।\n",
        "* **উদাহরণ:** Gradient Descent, Adam, RMSProp.\n",
        "\n",
        "---\n",
        "\n",
        "### সংক্ষেপে আমি বলতে পারি:\n",
        "ইনপুট ডেটা **ওয়েট** ও **বায়াস** এর মাধ্যমে প্রসেস হয়ে **অ্যাক্টিভেশন ফাংশন** দিয়ে বেরিয়ে যায়। এই পুরো প্রক্রিয়াকে আমরা বলি **Forward Propagation**। এরপর আমরা **লস ফাংশন** দিয়ে ভুল মাপি এবং **অপটিমাইজার** ব্যবহার করে সেই ভুল শুধরাই।"
      ],
      "metadata": {
        "id": "decuhXMjhImy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V2b08HOzhGVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. What is the difference between input layer, hidden layer, and output layer?"
      ],
      "metadata": {
        "id": "xBrzinFYhW71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ইনপুট, হিডেন এবং আউটপুট লেয়ারের মধ্যে পার্থক্য\n",
        "\n",
        "একটি নিউরাল নেটওয়ার্ক বা MLP মূলত তিনটি প্রধান ধরনের লেয়ার নিয়ে গঠিত। এদের প্রত্যেকের কাজ এবং বৈশিষ্ট্য আলাদা। আমি নিচে এদের পার্থক্যগুলো বিস্তারিত আলোচনা করছি:\n",
        "\n",
        "### ১. ইনপুট লেয়ার (Input Layer)\n",
        "এটি নেটওয়ার্কের একদম শুরুর লেয়ার।\n",
        "\n",
        "* **কাজ:** এই লেয়ারের প্রধান কাজ হলো বাইরের জগত থেকে ডেটা গ্রহণ করা। এটি অনেকটা আমাদের শরীরের \"চোখ\" বা \"কান\"-এর মতো কাজ করে।\n",
        "* **নিউরন সংখ্যা:** ইনপুট লেয়ারে কতগুলো নিউরন থাকবে তা নির্ভর করে আমাদের ইনপুট ডেটার ফিচারের সংখ্যার ওপর। যেমন: যদি আমি কারো বয়স, উচ্চতা এবং ওজন দিয়ে কোনো প্রেডিকশন করতে চাই, তবে ইনপুট লেয়ারে ৩টি নিউরন থাকবে।\n",
        "* **গাণিতিক হিসাব:** এই লেয়ারে সাধারণত কোনো ওয়েট গুণ বা অ্যাক্টিভেশন ফাংশনের মতো গাণিতিক হিসাব হয় না। এটি সরাসরি ডেটা পাস করে দেয়।\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ২. হিডেন লেয়ার (Hidden Layer)\n",
        "ইনপুট এবং আউটপুট লেয়ারের মাঝখানে যে এক বা একাধিক লেয়ার থাকে, সেগুলোই হলো হিডেন লেয়ার।\n",
        "\n",
        "* **কাজ:** একে নেটওয়ার্কের \"মস্তিষ্ক\" বলা যেতে পারে। এখানেই আসল শেখার প্রক্রিয়াটি ঘটে। এই লেয়ারগুলো ইনপুট ডেটার মধ্য থেকে জটিল প্যাটার্ন এবং ফিচার খুঁজে বের করে।\n",
        "* **নিউরন সংখ্যা:** হিডেন লেয়ারে কতগুলো নিউরন থাকবে তা আমরা (মডেল ডিজাইনার) ঠিক করি। এটি কোনো নির্দিষ্ট নিয়মের ওপর ভিত্তি করে নয়, বরং এক্সপেরিমেন্টের মাধ্যমে ঠিক করা হয়।\n",
        "* **গাণিতিক হিসাব:** এই লেয়ারেই ওয়েট ($w$) গুণ হয়, বায়াস ($b$) যোগ হয় এবং **অ্যাক্টিভেশন ফাংশন** (যেমন: ReLU) ব্যবহার করা হয়।\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ৩. আউটপুট লেয়ার (Output Layer)\n",
        "এটি নেটওয়ার্কের সর্বশেষ লেয়ার যা আমাদের চূড়ান্ত সিদ্ধান্ত বা ফলাফল দেয়।\n",
        "\n",
        "* **কাজ:** হিডেন লেয়ার থেকে আসা তথ্যগুলোকে প্রসেস করে এটি চূড়ান্ত প্রেডিকশন দেয়।\n",
        "* **নিউরন সংখ্যা:** এটি নির্ভর করে আমরা কী ধরনের সমস্যা সমাধান করছি তার ওপর।\n",
        "    - **Binary Classification:** ১টি নিউরন (হ্যাঁ/না বুঝাতে)।\n",
        "    - **Multi-class Classification:** ক্লাসের সংখ্যার সমান নিউরন (যেমন ১০টি সংখ্যা চিনতে হলে ১০টি নিউরন)।\n",
        "    - **Regression:** ১টি নিউরন (কোনো সংখ্যা প্রেডিক্ট করতে)।\n",
        "* **অ্যাক্টিভেশন ফাংশন:** এখানে সাধারণত আমাদের সমস্যার ওপর ভিত্তি করে **Sigmoid** (বাইনারি) বা **Softmax** (মাল্টিক্লাস) ব্যবহার করা হয়।\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### প্রধান পার্থক্যসমূহ একনজরে:\n",
        "\n",
        "| বৈশিষ্ট্য | ইনপুট লেয়ার | হিডেন লেয়ার | আউটপুট লেয়ার |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **কাজ** | ডেটা গ্রহণ করা | প্যাটার্ন এবং ফিচার শেখা | সিদ্ধান্ত বা প্রেডিকশন দেওয়া |\n",
        "| **গণনা** | কোনো হিসাব হয় না | ওয়েট, বায়াস ও অ্যাক্টিভেশন হিসাব হয় | চূড়ান্ত আউটপুট ও অ্যাক্টিভেশন হিসাব হয় |\n",
        "| **অবস্থান** | নেটওয়ার্কের শুরুতে | ইনপুট ও আউটপুটের মাঝে | নেটওয়ার্কের শেষে |\n",
        "| **সংখ্যা** | সব সময় ১টি থাকে | ১ বা তার বেশি (Deep Network) হতে পারে | সব সময় ১টি থাকে |\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition:\n",
        "আমি যদি একটি ছবি দেখে সেটি 'বিলাই' (Cat) কি না চিনতে চাই:\n",
        "- **ইনপুট লেয়ার:** ছবির পিক্সেলগুলো গ্রহণ করবে।\n",
        "- **হিডেন লেয়ার:** পিক্সেলগুলো বিশ্লেষণ করে কান, চোখ বা গোঁফ-এর মতো বৈশিষ্ট্যগুলো খুঁজবে।\n",
        "- **আউটপুট লেয়ার:** বলবে বিলাই হওয়ার সম্ভাবনা ৮০%।"
      ],
      "metadata": {
        "id": "s3kS7PfxhdXO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIA4sjF-hYik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. What is forward propagation?"
      ],
      "metadata": {
        "id": "o3sT7QYQhmIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ফরওয়ার্ড প্রপাগেশন (Forward Propagation) কী?\n",
        "\n",
        "ফরওয়ার্ড প্রপাগেশন হলো নিউরাল নেটওয়ার্কের সেই প্রক্রিয়া যার মাধ্যমে ইনপুট ডেটা নেটওয়ার্কের প্রতিটি লেয়ারের মধ্য দিয়ে প্রবাহিত হয়ে একটি চূড়ান্ত আউটপুট বা প্রেডিকশন তৈরি করে। আমি একে মডেলের \"অনুমান করার প্রক্রিয়া\" বলতে পারি।\n",
        "\n",
        "### ১. ফরওয়ার্ড প্রপাগেশন কীভাবে কাজ করে?\n",
        "\n",
        "এই প্রক্রিয়ায় তথ্য শুধুমাত্র সামনের দিকে (Forward direction) প্রবাহিত হয়। প্রতিটি নিউরনে মূলত দুটি প্রধান ধাপ সম্পন্ন হয়:\n",
        "\n",
        "**ধাপ ১: লিনিয়ার ক্যালকুলেশন (Linear Summation)**\n",
        "প্রতিটি ইনপুটের সাথে তার নির্দিষ্ট ওয়েট ($w$) গুণ করা হয় এবং সবশেষে একটি বায়াস ($b$) যোগ করা হয়।\n",
        "$$z = \\sum (w_i \\cdot a_i^{(l-1)}) + b$$\n",
        "\n",
        "**ধাপ ২: অ্যাক্টিভেশন (Activation)**\n",
        "প্রাপ্ত $z$ এর মানকে একটি নন-লিনিয়ার অ্যাক্টিভেশন ফাংশন ($\\sigma$) যেমন ReLU বা Sigmoid-এর মধ্য দিয়ে পাস করা হয়। এটিই হলো ওই নিউরনের আউটপুট ($a$), যা পরবর্তী লেয়ারের জন্য ইনপুট হিসেবে কাজ করে।\n",
        "$$a = \\sigma(z)$$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ২. ফরওয়ার্ড প্রপাগেশনের গাণিতিক ধাপসমূহ (Matrix Form)\n",
        "\n",
        "আমি যদি পুরো বিষয়টি ম্যাট্রিক্স আকারে চিন্তা করি, তবে প্রতিটি লেয়ারের জন্য হিসাবটি দাঁড়াবে এমন:\n",
        "\n",
        "**প্রথম হিডেন লেয়ারের জন্য:**\n",
        "$$z^{(1)} = W^{(1)} \\cdot X + b^{(1)}$$\n",
        "$$a^{(1)} = \\text{ReLU}(z^{(1)})$$\n",
        "\n",
        "**আউটপুট লেয়ারের জন্য:**\n",
        "$$z^{(2)} = W^{(2)} \\cdot a^{(1)} + b^{(2)}$$\n",
        "$$\\hat{y} = \\text{Sigmoid}(z^{(2)})$$\n",
        "\n",
        "এখানে $\\hat{y}$ হলো আমাদের মডেলের চূড়ান্ত প্রেডিকশন।\n",
        "\n",
        "---\n",
        "\n",
        "### ৩. সহজ উদাহরণ (Intuition)\n",
        "\n",
        "আমি যদি একটি সিস্টেম তৈরি করি যা নির্ধারণ করবে \"আজ আমি বাইরে খেলতে যাব কি না\", তবে ফরওয়ার্ড প্রপাগেশন এভাবে কাজ করবে:\n",
        "\n",
        "১. **ইনপুট:** আবহাওয়া কেমন? (যেমন: রোদ = ১, বৃষ্টি = ০) এবং আমার শরীর কেমন? (ভালো = ১, খারাপ = ০)।\n",
        "২. **প্রসেসিং (Hidden Layers):** আমার মস্তিষ্ক এই ইনপুটগুলোকে গুরুত্ব (Weights) দেবে। হয়তো খেলার জন্য আবহাওয়ার গুরুত্ব আমার শরীরের চেয়ে বেশি।\n",
        "৩. **আউটপুট:** সব হিসাব শেষে সিস্টেম আমাকে একটি প্রোবাবিলিটি দেবে, যেমন: ০.৮৫। এর মানে ৮৫% সম্ভাবনা যে আমি খেলতে যাব।\n",
        "\n",
        "---\n",
        "\n",
        "### ৪. ফরওয়ার্ড প্রপাগেশনের গুরুত্ব\n",
        "\n",
        "* **প্রেডিকশন:** ট্রেইনিং শেষে যখন আমরা নতুন ডেটা দিই, তখন ফরওয়ার্ড প্রপাগেশন ব্যবহার করেই মডেল রেজাল্ট দেয়।\n",
        "* **লস ক্যালকুলেশন:** ট্রেইনিংয়ের সময় ফরওয়ার্ড প্রপাগেশনের মাধ্যমে পাওয়া আউটপুটকে আসল উত্তরের (Target) সাথে তুলনা করে আমরা 'লস' বা ভুল বের করি। এই ভুল থেকেই পরে ব্যাকপ্রোপাগেশনের মাধ্যমে মডেল শেখে।\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Y3EMR7F2hp8L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WiHEk33ahn9L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}