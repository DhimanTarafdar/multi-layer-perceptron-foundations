{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7QWwbu+etONLCT7pcb3eo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhimanTarafdar/AAA/blob/main/Multi_layer_perceptron_basic_question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.What happens if we stack multiple linear layers without activation?"
      ],
      "metadata": {
        "id": "yf93Yj2EfsVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# কেন আমরা Deep Neural Network-এ শুধুমাত্র Linear Layer স্ট্যাক করি না?\n",
        "\n",
        "আমরা যখন ডিপ লার্নিং নিয়ে কাজ করি, তখন আমাদের মনে প্রশ্ন জাগতে পারে যে আমরা যদি অ্যাক্টিভেশন ফাংশন ছাড়া অনেকগুলো লিনিয়ার লেয়ার (Hidden Layers) একটার ওপর একটা বসাই, তবে কী হবে? বাস্তবে, **অ্যাক্টিভেশন ফাংশন ছাড়া মাল্টি-লেয়ার নেটওয়ার্ক আসলে একটি সিঙ্গেল লেয়ার নেটওয়ার্কের মতোই কাজ করে।**\n",
        "\n",
        "### ১. গাণিতিক বিশ্লেষণ (Mathematical Intuition)\n",
        "\n",
        "ধরি আমার একটি নেটওয়ার্ক আছে যার দুটি লেয়ার এবং কোনো অ্যাক্টিভেশন ফাংশন নেই।\n",
        "* প্রথম লেয়ারের আউটপুট: $h_1 = W_1 \\cdot X + b_1$\n",
        "* দ্বিতীয় লেয়ারের আউটপুট: $y = W_2 \\cdot h_1 + b_2$\n",
        "\n",
        "এখন যদি আমি $h_1$ এর মান দ্বিতীয় ইকুয়েশনে বসাই:\n",
        "$$y = W_2 \\cdot (W_1 \\cdot X + b_1) + b_2$$\n",
        "$$y = (W_2 \\cdot W_1) \\cdot X + (W_2 \\cdot b_1 + b_2)$$\n",
        "\n",
        "এখানে আমরা দেখতে পাচ্ছি যে, $(W_2 \\cdot W_1)$ গুণ হয়ে একটি নতুন ওয়েট ম্যাট্রিক্স $W_{new}$ তৈরি করছে এবং $(W_2 \\cdot b_1 + b_2)$ মিলে একটি নতুন বায়াস $b_{new}$ তৈরি করছে।\n",
        "আলটিমেটলি সমীকরণটি দাঁড়াচ্ছে:\n",
        "$$y = W_{new} \\cdot X + b_{new}$$\n",
        "\n",
        "এটি একটি সাধারণ **Linear Equation** বা **Single Layer Perceptron**-এর সমীকরণ। অর্থাৎ, আমরা মাঝখানে যতগুলো লেয়ারই দিই না কেন, গাণিতিকভাবে সেগুলো মিলেমিশে দিনশেষে একটি লেয়ারেই পরিণত হচ্ছে।\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ২. এটি কেন আমাদের জন্য সমস্যা?\n",
        "\n",
        "* **জটিল প্যাটার্ন শিখতে না পারা:** লিনিয়ার লেয়ার শুধুমাত্র সরলরেখা (Straight line) তৈরি করতে পারে। কিন্তু বাস্তব জগতের ডেটা (যেমন ইমেজ বা টেক্সট) অত্যন্ত জটিল এবং নন-লিনিয়ার। অ্যাক্টিভেশন ফাংশন ছাড়া আমাদের মডেল এই জটিলতা বুঝতেই পারবে না।\n",
        "* **নেটওয়ার্কের গভীরতা হারানো:** আমরা নেটওয়ার্ককে 'Deep' করি যেন প্রতি লেয়ারে নতুন নতুন বৈশিষ্ট্য (Features) শিখতে পারে। অ্যাক্টিভেশন ফাংশন না থাকলে লেয়ার বাড়িয়ে আমাদের কোনো লাভ নেই, কারণ মডেলটি লিনিয়ার রিগ্রেশন হিসেবেই আটকে থাকবে।\n",
        "\n",
        "---\n",
        "\n",
        "### ৩. সহজ উদাহরণ (Intuition)\n",
        "\n",
        "আমরা যদি অনেকগুলো স্বচ্ছ কাঁচের ওপর সোজা দাগ টানি এবং কাঁচগুলোকে একটার ওপর একটা রাখি, তবে আমরা দিনশেষে শুধুমাত্র সোজা দাগই দেখতে পাব। আমরা কখনোই একটি বৃত্ত বা আঁকাবাঁকা ম্যাপ তৈরি করতে পারব না। আমাদের এমন কিছু দরকার যা ওই সোজা দাগগুলোকে বাঁকিয়ে জটিল নকশা তৈরি করতে পারে। অ্যাক্টিভেশন ফাংশন ঠিক এই কাজটিই করে।\n",
        "\n",
        "---\n",
        "\n",
        "### সারসংক্ষেপ (Summary):\n",
        "১. **Collapsing Layers:** সব লেয়ার মিলে একটি মাত্র লিনিয়ার লেয়ারে পরিণত হয়।\n",
        "২. **Linearity:** আমরা শুধুমাত্র লিনিয়ার সমস্যা সমাধান করতে পারি।\n",
        "৩. **Useless Complexity:** লেয়ার বাড়ানো সত্ত্বেও আমাদের মডেলের ক্ষমতা বাড়ে না।\n",
        "\n",
        "**এই কারণেই আমাদের প্রতিটি লেয়ারের পর একটি Non-linear Activation Function (যেমন: ReLU বা Sigmoid) ব্যবহার করা আবশ্যিক।**"
      ],
      "metadata": {
        "id": "D1xeLsaIfx__"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DxfeA1HSfuhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. What kind of decision boundary does an MLP create?"
      ],
      "metadata": {
        "id": "QR61nqmngbcL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRSf79W4gc6T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}